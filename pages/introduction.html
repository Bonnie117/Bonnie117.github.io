<body>

  <img src="insert/neuralnetwork.jpeg" width="450">
  <p>
Our problem of interest for this project is to investigate ways to reduce the size of machine learning models while maintaining the same functionalities and performances as before. For the first half of the project, we investigated and studied BNNs through the paper “<a href="/insert/QNNBengio.pdf">Quantized Neural Networks</a>: Training Neural Networks with Low Precision Weights and Activations”. We focused on rebuilding the BNNs according to the method mentioned in the paper with more modernized libraries, namely Pytorch and Tensorflow, last quarter. In the second half of the project, we shifted our focus on investigating how does using different numbers of bits affects the accuracy of our models, especially for RNN and LSTM, since the paper mentioned how RNN and LSTM did not work well for 2 bits and needed 4 bits instead. Furthermore, we will also investigate how does using other functions, other than the HardTanh function, during backpropagation affect NNs’ accuracy, size, and power consumptions.
  </p>
</p>
  <img src="insert/brian.jpeg" width="350">
  <p>
    Our work is significant because we believe that the ability to utilize NNs on smaller devices will revolutionize the industry, and we would like to be part of this world-changing process while diving deep into studying some of the cutting-edge models and optimization methods.</p>
</body>
