<body>

  <h3>Datasets</h3>
  <p>We mainly utilized three common datasets for BNNs implementation:
    <li>MNIST</li>
    <li>SVHN</li>
    <li>CIFAR-10</li>
  </p>
  <p>And we further used IMDb dataset for exploration on LSTM.</p>

  <h3>Quarter 1</h3>
    <p>
In the first part of the project, we studied the methods introduced in the paper “Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations”. We also reproduced the methods and experiments described in the paper using two more modernized libraries, Tensorflow (<a href="https://github.com/lwilliams620/dsc180a-section/blob/main/180A%20-%20Q1%20Half%20Project/tensorflow_mnist_binary.py">TensorFlow Implementation</a> ) and PyTorch (<a href="https://github.com/lwilliams620/dsc180a-section/blob/main/180A%20-%20Q1%20Half%20Project/DSC180A-A12%20Pytorch.ipynb">PyTorch Implementation</a> ), instead of Theano which was what the paper originally used.
</p>
    <h4>Investigation & Analysis</h4>

    <h4>Results</h4>
      <h5>BNNs Accuracy</h5>
      <img src="insert/BNN.png" width="600">

    <!-- <p>
    <script src="echarts.js"></script>
    <div id="main" style="width: 600px;height:400px;"></div>
    <script type="text/javascript">
    var chartDom = document.getElementById('main');
    var myChart = echarts.init(chartDom);
    var option;

    option = {
      tooltip: {
        trigger: 'axis',
        axisPointer: {
          type: 'shadow'
        }
      },
      legend: {},
      grid: {
        left: '3%',
        right: '4%',
        bottom: '3%',
        containLabel: true
      },
      xAxis: {
        type: 'value',
        axisLabel: {
          formatter: '{value}%'
        },
        min: 0,
        max: 100,
        boundaryGap: [0, 0.01]
      },
      yAxis: {
        type: 'category',
        data: [
        'TensorFlow MNIST',
        'PyTorch MNIST',
        'Pytorch SVHN',
        'PyTorch CIFAR-10'
        ]
      },
      series: [
      {
        name: 'Binary Accuracy',
        type: 'bar',
        data: [96.46, 98.04, 83.02, 62.94],
        color: '#ffbcd9'
      },
      {
        name: 'FP Accuracy',
        type: 'bar',
        data: [98.57, 98.52, 86.42, 63.62]
      }
      ]
    };

    myChart.setOption(option);
    </script>
</p> -->

  <h3>Quarter 2</h3>
  <p>
During the second half of the project, we hope to explore how using different numbers of bits affects the accuracy of our models, especially for RNN and LSTM, since the paper mentioned how RNN and LSTM did not work well for 2 bits and needed 4 bits instead. Furthermore, we will also investigate how other functions, other than the HardTanh function, during backpropagation or a different combination of several layers and Neuron sizes might affect NNs’ accuracy, size, and power consumptions.
  </p>

  <h4>Analysis</h4>

  <h4>Results</h4>
  <h5>Qunatized MLP </h5>
  <p>Below are the results for our quantized MLP on the MNIST dataset. The full precision accuracy is 98.54%.</p>
  <img src="insert/QMLP.png" width="600">
  <h5>Quantized CNN: CIFAR-10</h5>
  <p>Below are the results for our quantized CNN on the CIFAR-10 dataset. The full precision accuracy is 81.54%.
</p>
  <img src="insert/QCIFAR10.png" width="600">
  <h5>Quantized CNN: SVHN</h5>
  <p>Below are the results for our quantized CNN on the SVHN dataset. The full precision accuracy is 94.02%</p>
  <img src="insert/QSVHN.png" width="600">
  <h5>Binarized LSTM</h5>
  <p>Below are the results for our quantized LSTM. The full precision accuracy is 87.66%.</p>
  <img src="insert/BLSTM.png" width="600">
</body>
