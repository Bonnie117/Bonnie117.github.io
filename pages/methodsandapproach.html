<body>

  <<h3>Datasets</h3>
  <p>We mainly utilized three common datasets for BNNs implementation:
    <li>MNIST</li>
    <li>SVHN</li>
    <li>CIFAR-10</li>
  </p>
  <p>And we further used IMBd dataset for exploration on LSTM.</p>

  <h3>Quarter 1</h3>
    <p>
In the first part of the project, we studied the methods introduced in the paper “Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations”. We also reproduced the methods and experiments described in the paper using two more modernized libraries, Tensorflow and PyTorch, instead of Theano which was what the paper originally used.
    <h4>Investigation & Analysis</h4>
    <h4>Results</h4>

</p>

  <h3>Quarter 2</h3>
  <p>
During the second half of the project, we hope to explore how using different numbers of bits affects the accuracy of our models, especially for RNN and LSTM, since the paper mentioned how RNN and LSTM did not work well for 2 bits and needed 4 bits instead. Furthermore, we will also investigate how other functions, other than the HardTanh function, during backpropagation or a different combination of several layers and Neuron sizes might affect NNs’ accuracy, size, and power consumptions.
  </p>
  
  <h4>Experiments</h4>
  <h4>Results</h4>

</body>
