<body>

  <h3>Datasets</h3>
  <p>We mainly utilized three common datasets for BNNs implementation:
    <li>MNIST</li>
    <li>SVHN</li>
    <li>CIFAR-10</li>
  </p>
  <p>And we further used IMDb dataset for exploration on LSTM.</p>

  <h3>Quarter 1</h3>
    <p>
In the first part of the project, we studied the methods introduced in the paper “Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations”. We also reproduced the methods and experiments described in the paper using two more modernized libraries, Tensorflow (<a href="https://github.com/lwilliams620/dsc180a-section/blob/main/180A%20-%20Q1%20Half%20Project/tensorflow_mnist_binary.py">TensorFlow Implementation</a>) and PyTorch (<a href="https://github.com/lwilliams620/dsc180a-section/blob/main/180A%20-%20Q1%20Half%20Project/DSC180A-A12%20Pytorch.ipynb">PyTorch Implementation</a>), instead of Theano which was what the paper originally used.
</p>
    <h4>Investigation & Analysis</h4>
    <p>Our baseline MLP uses dropout and batch normalization layers, along with three hidden dense layers, where each dense layer represents perceptron nodes. The parameters used in the model are the same as the paper we followed, with a couple of exceptions. The paper uses a decaying learning rate, starting at 0.003 and ending at 0.0000003. Our model uses a static learning rate of 0.001 because this achieved the best results. The paper also uses 1000 epochs to train; however, our model only uses 50. This is due to our model overfitting with more epochs.
    <p>Our binarized MLP uses the same layer setup, parameters, and inputs as the baseline model. However, we also implemented an early stopper with a patience level of 1 (i.e. the model stops training after it fails to reduce the validation loss) to ensure our model does not overfit. The dense layers are also changed so that they use the binarization methods described in the paper. We used the Larq library which modifies Tensorflow’s classes to binarize the weights with a sign function and compute the gradient with a straight-through estimator.
</p>
    <h4>Results</h4>
      <h5>BNNs Accuracy</h5>
      <img src="insert/BNN2.png" width="600">
      <p>Our Tensorflow version had an accuracy of 96.46% for the binary model and an accuracy of 98.57% for the full precision model for the MNIST dataset. The PyTorch version had an accuracy of 98.04% for the binary model and an accuracy of 98.52% for the full precision model for the MNIST dataset. Both versions of the implementation had a similar performance as the paper’s implementation for the full precision model (98.7%), but a lower accuracy for the binary version (99.04%). We believe that this discrepancy is due to the random number generator being used in Tensorflow and PyTorch, which might cause our versions of the model to select a smaller portion of the training data to be used as the validation set, causing the model to stop training earlier than the paper’s model.</p>

    <!-- <p>
    <script src="echarts.js"></script>
    <div id="main" style="width: 600px;height:400px;"></div>
    <script type="text/javascript">
    var chartDom = document.getElementById('main');
    var myChart = echarts.init(chartDom);
    var option;

    option = {
      tooltip: {
        trigger: 'axis',
        axisPointer: {
          type: 'shadow'
        }
      },
      legend: {},
      grid: {
        left: '3%',
        right: '4%',
        bottom: '3%',
        containLabel: true
      },
      xAxis: {
        type: 'value',
        axisLabel: {
          formatter: '{value}%'
        },
        min: 0,
        max: 100,
        boundaryGap: [0, 0.01]
      },
      yAxis: {
        type: 'category',
        data: [
        'TensorFlow MNIST',
        'PyTorch MNIST',
        'Pytorch SVHN',
        'PyTorch CIFAR-10'
        ]
      },
      series: [
      {
        name: 'Binary Accuracy',
        type: 'bar',
        data: [96.46, 98.04, 83.02, 62.94],
        color: '#ffbcd9'
      },
      {
        name: 'FP Accuracy',
        type: 'bar',
        data: [98.57, 98.52, 86.42, 63.62]
      }
      ]
    };

    myChart.setOption(option);
    </script>
</p> -->

  <h3>Quarter 2</h3>
  <p>
During the second half of the project, we hope to explore how using different numbers of bits affects the accuracy of our models, especially for RNN and LSTM, since the paper mentioned how RNN and LSTM did not work well for 2 bits and needed 4 bits instead. Furthermore, we will also investigate how other functions, other than the HardTanh function, during backpropagation or a different combination of several layers and Neuron sizes might affect NNs’ accuracy, size, and power consumptions.
  </p>

  <h4>Analysis</h4>
  <p>As shown in the comparison figures, Blue line is the quantized weights, orange line is the activation function. Left graph represents the original Hard-Tan function and right graph represents our modified Hyperbolic-Tan function. Our modification of activation function demonstrates a better optimization. </p>
  <img src="insert/Q2activation.png" width="600">

  <h4>Results</h4>
  <h5>Qunatized MLP </h5>
  <p>Below are the results for our quantized MLP on the MNIST dataset. The full precision accuracy is 98.54%.</p>
  <img src="insert/QMLP.png" width="600">
  <h5>Quantized CNN: CIFAR-10</h5>
  <p>Below are the results for our quantized CNN on the CIFAR-10 dataset. The full precision accuracy is 81.54%.
</p>
  <img src="insert/QCIFAR10.png" width="600">
  <h5>Quantized CNN: SVHN</h5>
  <p>Below are the results for our quantized CNN on the SVHN dataset. The full precision accuracy is 94.02%</p>
  <img src="insert/QSVHN.png" width="600">
  <h5>Binarized LSTM</h5>
  <p>Below are the results for our quantized LSTM. The full precision accuracy is 87.66%.</p>
  <img src="insert/BLSTM.png" width="600">
</body>
